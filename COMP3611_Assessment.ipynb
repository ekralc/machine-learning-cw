{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from scipy.stats import norm, t\n",
        "\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6b8qyKSypVL"
      },
      "source": [
        "## **Exercise 1**\n",
        "\n",
        "1.1 Consider some continuous random variables generated from an unknown distribution stored in 'clean_data.npy'. Fit a univariate Gaussian distribution to this data and estimate the mean and variance of the Gaussian distribution using the maximum likelihood estimator. Report the estimated mean and variance for the Gaussian distribution and plot its probability density function for continuous random variables in the range $[-10, 20]$. Overlay this probability density function curve on the normalised histogram of the data.\n",
        "\n",
        "**(5 marks)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfNaef4DydYn"
      },
      "outputs": [],
      "source": [
        "clean_data = np.load(\"clean_data.npy\").T\n",
        "\n",
        "N = clean_data.size\n",
        "\n",
        "mu, sigma = norm.fit(clean_data)\n",
        "\n",
        "print(\"mu:\", mu, \"sigma:\", sigma)\n",
        "\n",
        "x = np.linspace(-10, 20, 50)\n",
        "y = norm.pdf(x, mu, sigma)\n",
        "\n",
        "# Plot\n",
        "sns.histplot(clean_data, bins=50, stat='density')\n",
        "plt.plot(x, y, 'r')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ly2E3MLLD7I"
      },
      "source": [
        "1.2 Next, consider a 'corrupted' version of the data used in the previous exercise, stored in 'corrupted_data.npy'. This new data is affected by some degree of outliers from an unknown source. Repeat the process of fitting a univariate Gaussian distribution to this new data (using MLE) and report the estimated mean and variance of the distribution. Plot its probability density function for continuous random variables in the range $[-10, 35]$. Overlay this probability density function curve on the normalised histogram of the new data (affected by outliers). Comment on how the new Gaussian distribution parameters estimated have changed relative to the previous values estimated in exercise 1.1, and why.\n",
        "\n",
        "**(5 marks)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gs1pRFLQ4_Yt"
      },
      "outputs": [],
      "source": [
        "corrupted_data = np.load(\"corrupted_data.npy\")\n",
        "\n",
        "mu, sigma = norm.fit(corrupted_data)\n",
        "\n",
        "print(\"mu:\", mu, \"sigma:\", sigma)\n",
        "\n",
        "x = np.linspace(-10, 20, 50)\n",
        "y = norm.pdf(x, mu, sigma)\n",
        "\n",
        "# Plot\n",
        "sns.histplot(corrupted_data, bins=50, stat=\"density\")\n",
        "plt.plot(x, y, \"r\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comments\n",
        "The corrupted data is mostly the same as the clean data, apart from some new values which appear on the right and appear to be non-normally distributed.\n",
        "As a result $\\mu$ has been skewed higher, and the variance (\\$sigma$) has been increased.\n",
        "\n",
        "For this data, the MLE approach for fitting a Gaussian distribution is no longer suitable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFSH-cXiQkkm"
      },
      "source": [
        "1.3 Fit a distribution to the corrupted data from exercise 1.2 in a manner that is robust to the outliers present. Demonstrate this robustness by comparing the probability density functions of the robust and univariate Gaussian distribution for the corrupted data. Additionally compare the mean and variance estimated for both the clean data (from exercise 1.1) and the corrupted data (from exercise 1.2) based on the robust fit. Explain briefly, how your chosen approach to fitting a robust distribution to the corrupted data achieves robustness.\n",
        "\n",
        "**(5 marks)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSVp27ahNJI2"
      },
      "outputs": [],
      "source": [
        "# TODO Explanation\n",
        "\n",
        "corrupted_data = np.load(\"corrupted_data.npy\")\n",
        "\n",
        "_, mu, sigma = t.fit(clean_data)\n",
        "\n",
        "print(\"mu:\", mu, \"sigma:\", sigma)\n",
        "\n",
        "t_x = np.linspace(-10, 20, 50)\n",
        "t_y = norm.pdf(x, mu, sigma)\n",
        "\n",
        "# Plot\n",
        "sns.histplot(corrupted_data, bins=50, stat=\"density\")\n",
        "plt.plot(x, y, \"r\")\n",
        "plt.plot(t_x, t_y, \"g\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pwbCt5V9yEW"
      },
      "source": [
        "# **Exercise 2**\n",
        "\n",
        "2.1 You are given a data array called \"shape_array.npy\" that comprises 7 samples organised as columns in the array. Each column vector is a 3D shape of a blood vessel of size $(N\\times3)$ that has been reshaped into a vector of size $(N*3 \\times 1)$. Perform PCA (using the scikit-learn implementation) of the data array and extract the principal components (eigenvectors), the coordinates of the shapes in the new co-ordinate space defined by the eigenvectors, and the singular values associated with each of the eigenvectors.\n",
        "\n",
        "**(5 marks)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ssi9msrL3XyA"
      },
      "outputs": [],
      "source": [
        "samples = np.load(\"shape_array.npy\")\n",
        "\n",
        "samples.shape\n",
        "\n",
        "samples.T[0].reshape(-1, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Plot each vessel (assuming 'data' contains 7 vessels)\n",
        "for i in range(7):\n",
        "    vessel_data = samples.T[i].reshape(-1, 3)\n",
        "    x, y, z = vessel_data[:, 0], vessel_data[:, 1], vessel_data[:, 2]\n",
        "    \n",
        "    ax = fig.add_subplot(2, 4, i + 1, projection='3d')\n",
        "    ax.scatter(x, y, z, s=1, c='r')  # Plot the 3D coordinates of the vessel\n",
        "\n",
        "    ax.set_title(f'Vessel {i+1}')\n",
        "    ax.set_xlabel('X')\n",
        "    ax.set_ylabel('Y')\n",
        "    ax.set_zlabel('Z')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pca = PCA(n_components=3)\n",
        "pca.fit(samples.T)\n",
        "\n",
        "# Eigenvectors\n",
        "pca.components_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Eigenvalues\n",
        "pca.explained_variance_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Samples in new coordinate space compressed to 3 dimensions\n",
        "pca_array = pca.transform(samples.T)\n",
        "\n",
        "pca_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvQFz0nRBVt7"
      },
      "source": [
        "2.2 Next, perform eigendecomposition of the covariance matrix estimated from the given data array. Compare the obtained eigenvalues with the singular values estimated from PCA in the previous step. Report any differences you might find between the two and briefly explain the reason for any differences. Find the new coordinates of each shape (i.e. column in the data array) in the new coordinate space defined by the estimated eigenvectors.\n",
        "\n",
        "**(5 marks)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "M = np.mean(samples.T, axis=0)\n",
        "\n",
        "# Center the samples around 0\n",
        "samples_standardised = (samples.T - M).T\n",
        "cov = np.cov(samples_standardised)\n",
        "\n",
        "values, vectors = np.linalg.eig(cov)\n",
        "\n",
        "components = vectors[:,:3].real"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "samples_standardised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eig_array = np.matmul(samples_standardised.T, components)\n",
        "\n",
        "eig_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pca_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Comment:** There appears to be no differences in coordinates other than the sign"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVhp6bCvCyp4"
      },
      "source": [
        "2.3 Reconstruct any one shape from the provided data array using (a) new coordinates estimated from PCA in 2.1 and (b) the new coordinates estimated using eigendecomposition in 2.2. Reshape the resulting vectors from (a) and (b) into a 3D set of points of size $(N\\times3)$ that represent reconstructions of the original shape. Overlay the two resulting shapes and briefly comment on their similarity. Finally, in a couple of sentences explain why PCA is often described as an approach for dimensionality reduction/data compression.\n",
        "\n",
        "**(5 marks)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform the inverse of the previous step and uncenter the data using the mean matrix\n",
        "reconstructed = np.matmul(eig_array, np.linalg.pinv(components)) + M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "\n",
        "reconstructed_vessel = reconstructed[0].T.reshape(-1, 3)\n",
        "print(reconstructed_vessel)\n",
        "x, y, z = reconstructed_vessel[:, 0], reconstructed_vessel[:, 1], reconstructed_vessel[:, 2]\n",
        "ax.scatter(x, y, z, s=1, c='r')  # Plot the 3D coordinates of the vessel\n",
        "\n",
        "reconstructed_vessel = pca.inverse_transform(pca_array)[0].T.reshape(-1, 3)\n",
        "print(reconstructed_vessel)\n",
        "x, y, z = reconstructed_vessel[:, 0], reconstructed_vessel[:, 1], reconstructed_vessel[:, 2]\n",
        "ax.scatter(x, y, z, s=2, c='b')  # Plot the 3D coordinates of the vessel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Comment:** The reconstructed vessels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUdbpHtSF91U"
      },
      "source": [
        "# **Exercise 3: Predict Cancer Mortality Rates in US Counties**\n",
        "\n",
        "The provided dataset comprises data collected from multiple counties in the US. The regression task for this assessment is to predict cancer mortality rates in \"unseen\" US counties, given some training data. The training data ('Training_data.csv') comprises various features/predictors related to socio-economic characteristics, amongst other types of information for specific counties in the country. The corresponding target variables for the training set are provided in a separate CSV file ('Training_data_targets.csv'). Use the notebooks provided for lab sessions throughout this module to provide solutions to the exercises listed below. Throughout all exercises text describing your code and answering any questions included in the exercise descriptions should be included as part of your submitted solution.\n",
        "\n",
        "\n",
        "The list of predictors/features available in this data set are described below:\n",
        "\n",
        "**Data Dictionary**\n",
        "\n",
        "avgAnnCount: Mean number of reported cases of cancer diagnosed annually\n",
        "\n",
        "avgDeathsPerYear: Mean number of reported mortalities due to cancer\n",
        "\n",
        "incidenceRate: Mean per capita (100,000) cancer diagoses\n",
        "\n",
        "medianIncome: Median income per county\n",
        "\n",
        "popEst2015: Population of county\n",
        "\n",
        "povertyPercent: Percent of populace in poverty\n",
        "\n",
        "MedianAge: Median age of county residents\n",
        "\n",
        "MedianAgeMale: Median age of male county residents\n",
        "\n",
        "MedianAgeFemale: Median age of female county residents\n",
        "\n",
        "AvgHouseholdSize: Mean household size of county\n",
        "\n",
        "PercentMarried: Percent of county residents who are married\n",
        "\n",
        "PctNoHS18_24: Percent of county residents ages 18-24 highest education attained: less than high school\n",
        "\n",
        "PctHS18_24: Percent of county residents ages 18-24 highest education attained: high school diploma\n",
        "\n",
        "PctSomeCol18_24: Percent of county residents ages 18-24 highest education attained: some college\n",
        "\n",
        "PctBachDeg18_24: Percent of county residents ages 18-24 highest education attained: bachelor's degree\n",
        "\n",
        "PctHS25_Over: Percent of county residents ages 25 and over highest education attained: high school diploma\n",
        "\n",
        "PctBachDeg25_Over: Percent of county residents ages 25 and over highest education attained: bachelor's degree\n",
        "\n",
        "PctEmployed16_Over: Percent of county residents ages 16 and over employed\n",
        "\n",
        "PctUnemployed16_Over: Percent of county residents ages 16 and over unemployed\n",
        "\n",
        "PctPrivateCoverage: Percent of county residents with private health coverage\n",
        "\n",
        "PctPrivateCoverageAlone: Percent of county residents with private health coverage alone (no public assistance)\n",
        "\n",
        "PctEmpPrivCoverage: Percent of county residents with employee-provided private health coverage\n",
        "\n",
        "PctPublicCoverage: Percent of county residents with government-provided health coverage\n",
        "\n",
        "PctPubliceCoverageAlone: Percent of county residents with government-provided health coverage alone\n",
        "\n",
        "PctWhite: Percent of county residents who identify as White\n",
        "\n",
        "PctBlack: Percent of county residents who identify as Black\n",
        "\n",
        "PctAsian: Percent of county residents who identify as Asian\n",
        "\n",
        "PctOtherRace: Percent of county residents who identify in a category which is not White, Black, or Asian\n",
        "\n",
        "PctMarriedHouseholds: Percent of married households\n",
        "\n",
        "BirthRate: Number of live births relative to number of women in county"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQOJmSpWGKhZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "root_dir = os.getcwd() + \"/\"\n",
        "\n",
        "## Define paths to the training data and targets files\n",
        "training_data_path = root_dir + 'Training_data.csv'\n",
        "training_targets_path = root_dir + 'Training_data_targets.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV90xYdHGVOw"
      },
      "source": [
        "**Exercise 3.1**\n",
        "\n",
        "Read in the training data and targets files. The training data comprises features/predictors while the targets file comprises the targets (i.e. cancer mortality rates in US counties) you need to train models to predict. Plot histograms of all features to visualise their distributions and identify outliers. Do you notice any unusual values for any of the features? If so comment on these in the text accompanying your code. Compute correlations of all features with the target variable (across the data set) and sort them according the strength of correlations. Which are the top five features with strongest correlations to the targets? Plot these correlations using the scatter matrix plotting function available in pandas and comment on at least two sets of features that show visible correlations to each other.\n",
        "\n",
        "**(5 marks)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAULU3j6Gh1l"
      },
      "outputs": [],
      "source": [
        "df_features = pd.read_csv(training_data_path)\n",
        "df_target = pd.read_csv(training_targets_path)\n",
        "\n",
        "df = pd.concat([df_features, df_target], axis=1)\n",
        "\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.drop(['PctSomeCol18_24', 'PctPrivateCoverageAlone'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Unusual Values\n",
        "\n",
        "- `PctSomeCol18_24` has mostly null values, so it is excluded.\n",
        "- `PctPrivateCoverageAlone` is also excluded, as it has a large number of missing values.\n",
        "- `PctEmployed16_Over` has 119 missing values, which are replaced with the mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.hist(bins=100,figsize=(20,15))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df[df['MedianAge'] < 100]\n",
        "\n",
        "# Reset the split dataframes after removing the rows\n",
        "df_features = df.drop(['TARGET_deathRate'], axis=1)\n",
        "df_target = df[['TARGET_deathRate']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyJ1nNgkpSQX"
      },
      "source": [
        "\n",
        "*   There seem to be errors/outliers in the median age features (MedianAge) with values >> 100. This is clearly an error and needs to be corrected prior to fitting regression models. (1.5 marks for code above and this discussion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWMHK5T0pNr1"
      },
      "outputs": [],
      "source": [
        "abs_corr = abs(df.corrwith(df['TARGET_deathRate']))\n",
        "\n",
        "abs_corr.sort_values(ascending=False).head(n=6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJ94VlYhpXyp"
      },
      "source": [
        "*   Top five features with strongest correlations to targets are: incidenceRate, PctBachDeg25_Over, PctPublicCoverageAlone, medIncome and povertyPercent (2 marks for this description and code above).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fK9qQbCUpbMa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnTR6yj9pe-k"
      },
      "source": [
        "*   medIncome and povertyPercent are negatively correlated to each other as you would expect.\n",
        "*   povertyPercent and PctBachDeg25_Over are also negatively correlated highlighting that counties with higher degrees of poverty have fewer Bachelor graduates by the age of 25. povertyPercent also shows a strong positive correlation with PctPublicCoverageAlone, indicating that poverty stricken counties are less likely to be able to afford private healthcare coverage.\n",
        "*   Similarly, PctBachDeg25_Over is negatively correlated with PctPublicCoverageAlone and positively correlated with medIncome. (1.5 marks for discussion of at least two sets of features that show correlations and code above)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdVWolDApj4g"
      },
      "source": [
        "**Exercise 3.2**\n",
        "\n",
        "Create an ML pipeline using scikit-learn (as demonstrated in the lab notebooks) to pre-process the training data. (5 marks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcbcQArap9l1"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('std_scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "features_tr = pipeline.fit_transform(df_features)\n",
        "\n",
        "# Preview results\n",
        "pd.DataFrame(features_tr, columns=df_features.columns).head()\n",
        "\n",
        "print(features_tr.shape)\n",
        "print(df_target.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5ZReCbuqBFP"
      },
      "source": [
        "**Exercise 3.3**\n",
        "\n",
        "Fit linear regression models to the pre-processed data using: Ordinary least squares (OLS), Lasso and Ridge models. Choose suitable regularisation weights for Lasso and Ridge regression and include a description in text of how they were chosen. In your submitted solution make sure you set the values for the regularisation weights equal to those you identify from your experiment(s). Quantitatively compare your results from all three models and report the best performing one. Report the overall performance of the best regression model identified. Include code for all steps above. (10 marks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_features, df_target, test_size=0.333, random_state=42)\n",
        "\n",
        "X_train_tr = pipeline.transform(X_train)\n",
        "X_test_tr = pipeline.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### OLS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngigS4ZQqGUw"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_train_tr, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "def evaluate(estimator, X, y):\n",
        "    pred = estimator.predict(X)\n",
        "    mse = mean_squared_error(y, pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y, pred)\n",
        "\n",
        "    print(\"MSE:\", mse)\n",
        "    print(\"RMSE:\",rmse)\n",
        "    print(\"r2:\",r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluate(lin_reg, X_test_tr, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "scores = cross_validate(lin_reg, features_tr, df_target, cv=10, scoring='neg_mean_squared_error')\n",
        "\n",
        "scores['test_score']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Lasso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LassoCV\n",
        "\n",
        "lasso_reg = LassoCV(cv=10)\n",
        "lasso_reg.fit(X_train_tr, y_train.values.ravel())\n",
        "\n",
        "evaluate(lasso_reg, X_test_tr, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-fLtDtYqVRq"
      },
      "source": [
        "#### Ridge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import RidgeCV\n",
        "\n",
        "ridge_reg = RidgeCV(cv=10)\n",
        "ridge_reg.fit(X_train_tr, y_train.values.ravel())\n",
        "\n",
        "evaluate(ridge_reg, X_test_tr, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "scores = cross_validate(lasso_reg, X_test_tr, y_test.values.ravel(), scoring=\"neg_mean_squared_error\", cv=10, return_estimator=True)\n",
        "\n",
        "print(scores['test_score'])\n",
        "scores['test_score'].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bestIdx = np.argmin(scores['score_time'])\n",
        "estimator = scores['estimator'][bestIdx]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.10 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "ac72a095e86e94b1aeecba2a4c17072b1cb29610b2bed48409ab196c57449b33"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
